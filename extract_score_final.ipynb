{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow:\n",
    "# For each document:\n",
    "# 1. Preprocessing and extract features using RegEx\n",
    "# 2. Split into overlapping chunks.\n",
    "# 3. Return the most important chunks using Memwalker\n",
    "# 4. Concatenate the most important chunks.\n",
    "# 5. If a chunk is too long, shorten it by summarising its chunks.\n",
    "# 6. Use text and features to receive a financial mismanagement score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "KEY = \"YOUR_KEY\" # add your gemini api key\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove non-ASCII characters, normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # remove non-ASCII\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', ' ', text)\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Load spaCy model (run python -m spacy download en_core_web_sm first if needed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_financial_features(text):\n",
    "    \"\"\"\n",
    "    Extract financial features from a single NHS trust audit report.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The audit report text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of extracted features with normalized values\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    features = {}\n",
    "    \n",
    "    # Helper function to normalize amounts to millions\n",
    "    def normalize_amount(amount_str):\n",
    "        if not amount_str:\n",
    "            return 0.0\n",
    "        clean_str = re.sub(r'[^\\d.]', '', amount_str)\n",
    "        if not clean_str:\n",
    "            return 0.0\n",
    "        amount = float(clean_str)\n",
    "        if 'thousand' in amount_str.lower():\n",
    "            return amount / 1000\n",
    "        elif 'million' in amount_str.lower():\n",
    "            return amount\n",
    "        elif 'billion' in amount_str.lower() or 'bn' in amount_str.lower():\n",
    "            return amount * 1000\n",
    "        else:\n",
    "            return amount / 1e6\n",
    "    \n",
    "    # 1. Financial amounts\n",
    "    amounts = re.findall(r'£\\s*(\\d{1,3}(?:,\\d{3})(?:\\.\\d{1,2})?|\\d+\\s(?:million|billion|bn|m|thousand)\\b)', text, re.IGNORECASE)\n",
    "    normalized_amounts = [normalize_amount(amt) for amt in amounts]\n",
    "    features['total_amount_mentioned'] = sum(normalized_amounts)\n",
    "    features['num_amount_mentions'] = len(normalized_amounts)\n",
    "    \n",
    "    # 2. Context-specific amounts\n",
    "    context_patterns = {\n",
    "        'deficit_amount': r'(?:deficit|shortfall)\\D{1,20}£\\s*([\\d,\\.]+)',\n",
    "        'savings_amount': r'(?:savings|efficiency)\\D{1,20}£\\s*([\\d,\\.]+)',\n",
    "        'overspend_amount': r'(?:overspend|exceed)\\D{1,20}£\\s*([\\d,\\.]+)'\n",
    "    }\n",
    "    for name, pattern in context_patterns.items():\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        features[name] = normalize_amount(matches[0]) if matches else 0.0\n",
    "    \n",
    "    # 3. Risk indicators\n",
    "    risk_terms = ['risk', 'concern', 'challenge', 'weakness', 'vulnerability']\n",
    "    features['risk_count'] = sum(1 for token in doc if token.lemma_.lower() in risk_terms)\n",
    "    \n",
    "    # 4. Governance indicators\n",
    "    governance_terms = ['governance', 'control', 'oversight', 'compliance']\n",
    "    features['governance_count'] = sum(1 for token in doc if token.lemma_.lower() in governance_terms)\n",
    "    \n",
    "    # 5. Performance metrics\n",
    "    features['kpi_mentions'] = len(re.findall(r'\\b(?:performance\\s+)?(target|metric|indicator|kpi|benchmark)\\b', text, re.IGNORECASE))\n",
    "    \n",
    "    # 6. Negative sentiment\n",
    "    negative_terms = ['fail', 'inadequate', 'poor', 'weak', 'insufficient']\n",
    "    features['negative_count'] = sum(1 for token in doc if token.lemma_.lower() in negative_terms)\n",
    "    \n",
    "    # 7. Audit opinion\n",
    "    opinion_match = re.search(r'\\b(qualified\\s+opinion|adverse\\s+opinion|disclaimer|emphasis\\s+of\\s+matter)\\b', text, re.IGNORECASE)\n",
    "    features['audit_opinion'] = opinion_match.group(0) if opinion_match else 'unqualified'\n",
    "    \n",
    "    # 8. Sentence-level analysis\n",
    "    deficit_sents = [sent.text for sent in doc.sents if any(t.lemma_.lower() in {'deficit', 'shortfall'} for t in sent)]\n",
    "    features['deficit_sentences'] = len(deficit_sents)\n",
    "    \n",
    "    # 9. Named entities\n",
    "    money_ents = [ent.text for ent in doc.ents if ent.label_ == 'MONEY']\n",
    "    features['money_entities'] = len(money_ents)\n",
    "    \n",
    "    # 10. Normalized scores\n",
    "    total_words = len([t for t in doc if t.is_alpha])\n",
    "    features['risk_density'] = features['risk_count'] / total_words if total_words > 0 else 0\n",
    "    features['negative_density'] = features['negative_count'] / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess_and_extract_features(text):\n",
    "    cleaned = preprocess(text)\n",
    "    features = extract_financial_features(cleaned)\n",
    "    return cleaned, features\n",
    "\n",
    "def split_overlapping_chunks(text, chunk_size=2500, overlap=50): # some reports might need smaller chunk_size to work\n",
    "    \"\"\"\n",
    "    Split `text` into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to split.\n",
    "        chunk_size (int): The maximum length of each chunk (must be > 0).\n",
    "        overlap (int): The number of characters by which consecutive chunks overlap (0 <= overlap < chunk_size).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of string chunks.\n",
    "    \"\"\"\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be > 0\")\n",
    "    if not (0 <= overlap < chunk_size):\n",
    "        raise ValueError(\"overlap must satisfy 0 <= overlap < chunk_size\")\n",
    "\n",
    "    step = chunk_size - overlap\n",
    "    chunks = []\n",
    "    for start in range(0, len(text), step):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        if end >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Memwalker code\n",
    "@dataclass\n",
    "class Node:\n",
    "    summary:  Optional[str]      = None      # internal summaries\n",
    "    text:     Optional[str]      = None      # raw passage for leaves\n",
    "    children: List[\"Node\"]       = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self) -> bool:\n",
    "        return not self.children\n",
    "\n",
    "def build_memory_tree(text,\n",
    "                      client,\n",
    "                      branching = 4):\n",
    "    \"\"\"\n",
    "    1. Chunk long text into leaves.\n",
    "    2. Recursively group `branching` children and ask the LLM to\n",
    "       summarize them, building parent nodes until a single root remains.\n",
    "    \"\"\"\n",
    "    # ---- 1-a. Create leaf level -----------------\n",
    "    leaves = [Node(text=chunk) for chunk in\n",
    "              split_overlapping_chunks(text)]\n",
    "    level = leaves\n",
    "\n",
    "    # ---- 1-b. Bottom-up summarization -----------\n",
    "    def _summarize(chunks):\n",
    "        joined = \"\\n\\n\".join(chunks)\n",
    "        return summarize_chunk(joined, client) \n",
    "\n",
    "    while len(level) > 1:\n",
    "        parents = []\n",
    "        for i in range(0, len(level), branching):\n",
    "            group = level[i:i + branching]\n",
    "            parent_summary = _summarize(\n",
    "                [c.text if c.is_leaf else c.summary for c in group]\n",
    "            )\n",
    "            parents.append(Node(summary=parent_summary,\n",
    "                                children=group))\n",
    "        level = parents                      # climb one level\n",
    "    return level[0]                          # root\n",
    "\n",
    "\n",
    "DECISION_TEMPLATE = \"\"\"You are an auditor searching a document tree for\n",
    "evidence of \"financial mismanagement\". Below is the current node’s summary followed by its\n",
    "child summaries.\n",
    "\n",
    "Current summary:\n",
    "{parent_summary}\n",
    "\n",
    "Child summaries (numbered):\n",
    "{numbered_children}\n",
    "\n",
    "Which child(ren) should be opened next? Reply with *comma-separated indices*\n",
    "(e.g. \"2\" or \"1,3\") or \"none\" if none seem relevant.\n",
    "\"\"\"\n",
    "\n",
    "def _preview(node: Node, n_chars: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Return a short snippet for the navigation prompt:\n",
    "      • use node.summary if it exists\n",
    "      • otherwise fall back to node.text\n",
    "    \"\"\"\n",
    "    src = node.summary if node.summary is not None else node.text\n",
    "    return (src or \"\")[:n_chars] + \"…\"\n",
    "\n",
    "def choose_children_via_llm(parent, client):\n",
    "    numbered = \"\\n\".join(\n",
    "        f\"{idx}. {_preview(child)}\"\n",
    "        for idx, child in enumerate(parent.children, 1)\n",
    "    )\n",
    "    prompt = DECISION_TEMPLATE.format(\n",
    "        parent_summary=parent.summary,\n",
    "        numbered_children=numbered\n",
    "    )\n",
    "    reply = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)         \n",
    "    ).text.lower().strip()\n",
    "    if \"none\" in reply:\n",
    "        return []\n",
    "    # extract integers 1-based → 0-based\n",
    "    return [int(i) - 1 for i in reply.split(\",\") if i.strip().isdigit()]\n",
    "\n",
    "def walk_memory_tree(root,\n",
    "                     client,\n",
    "                     max_hops = 10):\n",
    "    \"\"\"\n",
    "    Depth-first walk driven by LLM decisions.\n",
    "    Returns the *leaf* nodes visited in order.\n",
    "    \"\"\"\n",
    "    stack   = [(root, 0)]      # node, depth\n",
    "    visited = []\n",
    "\n",
    "    while stack and len(visited) < max_hops:\n",
    "        node, depth = stack.pop()\n",
    "        if node.is_leaf:\n",
    "            visited.append(node)\n",
    "            continue\n",
    "        # Ask which branches to open\n",
    "        choices = choose_children_via_llm(node, client)\n",
    "        # Push chosen children to the stack (LIFO → DFS)\n",
    "        for idx in reversed(choices):\n",
    "            stack.append((node.children[idx], depth + 1))\n",
    "    return visited             # ordered path of relevant leaves\n",
    "\n",
    "def memwalker_retrieve(text,\n",
    "                       client,\n",
    "                       branching   = 4,\n",
    "                       max_hops    = 10):\n",
    "    \"\"\"\n",
    "    One-shot helper that:\n",
    "       • builds the memory tree,\n",
    "       • walks it,\n",
    "       • returns the *raw text* of visited leaves.\n",
    "    \"\"\"\n",
    "    root = build_memory_tree(text, client,\n",
    "                               branching=branching)\n",
    "    leaves = walk_memory_tree(root, client, max_hops=max_hops)\n",
    "    return [leaf.text for leaf in leaves]\n",
    "\n",
    "# Not used in final model\n",
    "def rank_chunks_tfidf(chunks, query=\"financial mismanagement\", top_k=10):\n",
    "    \"\"\"\n",
    "    Ranks text chunks by their relevance to a query using TF-IDF + cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks: list of str, the document chunks.\n",
    "    - query: str, the concept to rank against.\n",
    "    - top_k: int, the number of top chunks to return.\n",
    "    \n",
    "    Returns:\n",
    "    - List of indices of the top_k most relevant chunks.\n",
    "    \"\"\"\n",
    "    # Fit TF-IDF on both chunks and the query\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(chunks + [query])\n",
    "    \n",
    "    # Separate chunk vectors and the query vector\n",
    "    chunk_vectors = tfidf_matrix[:-1]\n",
    "    query_vector = tfidf_matrix[-1]\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(query_vector, chunk_vectors)[0]\n",
    "    \n",
    "    # Select top_k chunk indices\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "    return top_indices.tolist()\n",
    "\n",
    "def summarize_chunk(chunk, client):\n",
    "    prompt = (\n",
    "        \"Please summarize the following section:\\n\\n\"\n",
    "        f\"{chunk}\"\n",
    "    )\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents= prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)).text    \n",
    "    return resp.strip()\n",
    "\n",
    "#  Synthesize final summary \n",
    "def synthesize_summaries(summaries, client):\n",
    "    combined = \"\\n\\n\".join(f\"- {s}\" for s in summaries)\n",
    "    prompt = (\n",
    "        \"Here are summaries of the relevant sections:\\n\\n\"\n",
    "        f\"{combined}\\n\\n\"\n",
    "        \"Please produce a cohesive executive summary highlighting the key points.\"\n",
    "    )\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents= prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)         \n",
    "    ).text     \n",
    "    return resp.strip()\n",
    "\n",
    "def summary_to_score(summary, features_1, features_2, client):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents= f\"\"\"\n",
    "    You are an auditor evaluating the attached summary of an NHS Trust audit report. Also consider some extracted features from the report. \n",
    "    Output only a single JSON object with:\n",
    "    - \"financial_mismanagement_score\": a numeric value (0.0 to 1.0) for overall likelihood of financial mismanagement or fraud (assume a lower risk if much is redacted/unconfirmed).\n",
    "\n",
    "    Do not add any commentary or explanation beyond this JSON structure. Do this in a step-by-step manner.\n",
    "    Here is the report: {summary} and the features {features_1}, {features_2}.\n",
    "    \"\"\",\n",
    "    config=types.GenerateContentConfig(temperature=0)         \n",
    "    ).text \n",
    "    \n",
    "    start_index = response.index('{')\n",
    "    end_index = response.rindex('}') + 1\n",
    "    json_content = response[start_index:end_index]\n",
    "\n",
    "    # 2. Load as JSON.\n",
    "    data = json.loads(json_content)\n",
    "\n",
    "    financial_mismanagement_score = data[\"financial_mismanagement_score\"]   \n",
    "    return financial_mismanagement_score \n",
    "\n",
    "# didn't use it in final model\n",
    "def summary_features_to_factsheet(summary, features, excel_features, client):\n",
    "    EXTRACT_PROMPT = \"\"\"\n",
    "You are a forensic accountant.\n",
    "\n",
    "Task 1 (critical – do not skip):\n",
    "Read the document and the features and fill the 26 JSON fields below.\n",
    "If a fact is missing, use null.\n",
    "\n",
    "Return **only** valid JSON – no commentary.\n",
    "\n",
    "Fields:\n",
    "{{\n",
    "    \"trust_name\": \"\",                    # e.g. \"Acme Trust\"\n",
    "    \"fy_start\": \"\",                       # \"2019-04-01\"\n",
    "    \"fy_end\": \"\",                         # \"2020-03-31\"\n",
    "    \"audit_opinion\": \"\",                  # qualified | unqualified | adverse\n",
    "    \"section_24_report\": null,            # true | false\n",
    "    \"statutory_breakeven_breach\": null,   # true | false\n",
    "    \"unlawful_expenditure\": null,         # true | false\n",
    "    \"use_of_resources_rating\": \"\",        # good | requires improvement | inadequate\n",
    "    \"annual_deficit_m£\": null,            # float, millions GBP\n",
    "    \"cumulative_loans_m£\": null,          # float, millions GBP\n",
    "    \"OpinionOnFinancialStatements\": \"\",   # standard | non-standard\n",
    "    \"ValueformoneyArrangementsConclusion\": \"\",  # qualified | unqualified\n",
    "    \"AdditionalReportingPowers\": \"\",      # exercised | not exercised\n",
    "    \"total_amount_mentioned\": null,       # GBP total figures\n",
    "    \"num_amount_mentions\": null,          # integer count\n",
    "    \"deficit_amount\": null,               # GBP deficit figure\n",
    "    \"savings_amount\": null,               # GBP savings figure\n",
    "    \"overspend_amount\": null,             # GBP overspend figure\n",
    "    \"risk_count\": null,                   # number of risks called out\n",
    "    \"governance_count\": null,             # governance issues count\n",
    "    \"kpi_mentions\": null,                 # count of KPI mentions\n",
    "    \"negative_count\": null,               # count of negative words\n",
    "    \"deficit_sentences\": null,            # number of sentences about deficit\n",
    "    \"money_entities\": null,               # named monetary entities\n",
    "    \"risk_density\": null,                 # float per 1k words\n",
    "    \"negative_density\": null              # float per 1k words\n",
    "}}\n",
    "\n",
    "<BEGIN_FEATURES>\n",
    "{features_1}\n",
    "{features_2}\n",
    "<END_FEATURES>\n",
    "\n",
    "<BEGIN_DOCUMENT>\n",
    "{document}\n",
    "<END_DOCUMENT>\n",
    "\"\"\"\n",
    "    prompt = EXTRACT_PROMPT.format(\n",
    "        document=summary,\n",
    "        features_1=excel_features,\n",
    "        features_2=features\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)         \n",
    "    ).text\n",
    "    return response\n",
    "\n",
    "\n",
    "def summarize_full_text(text, client, chunk_size=2000):\n",
    "    \"\"\"\n",
    "    Splits `text` into chunks of up to `chunk_size` characters (splitting on word boundaries),\n",
    "    summarises each chunk, and then synthesises an ~800-word executive summary.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full document to summarise.\n",
    "        client: An LLM client\n",
    "        chunk_size (int): Maximum characters per chunk (default: 2000).\n",
    "        \n",
    "    Returns:\n",
    "        str: A cohesive ~800-word executive summary.\n",
    "    \"\"\"\n",
    "    # 1. Split text into word-safe chunks\n",
    "    words = text.split()\n",
    "    chunks, current = [], \"\"\n",
    "    for w in words:\n",
    "        # +1 for the space we’ll add\n",
    "        if len(current) + len(w) + 1 <= chunk_size:\n",
    "            current += (\" \" if current else \"\") + w\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = w\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    # 2. Summarise each chunk\n",
    "    summaries = []\n",
    "    for idx, chunk in enumerate(chunks, start=1):\n",
    "        prompt = (\n",
    "            f\"Chunk {idx} of {len(chunks)}:\\n\\n\"\n",
    "            f\"{chunk}\\n\\n\"\n",
    "            \"Please provide a concise summary (around 150 words) of the above.\"\n",
    "        )\n",
    "        resp = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(temperature=0)            \n",
    "        ).text\n",
    "        summaries.append(resp.strip())\n",
    "\n",
    "    # 3. Combine chunk summaries and request final 800-word summary\n",
    "    combined = \"\\n\\n\".join(f\"- {s}\" for s in summaries)\n",
    "    final_prompt = (\n",
    "        \"Here are summaries of each section:\\n\\n\"\n",
    "        f\"{combined}\\n\\n\"\n",
    "        \"Please produce a single, cohesive executive summary of approximately 700 words, \"\n",
    "        \"highlighting the key points across all sections.\"\n",
    "    )\n",
    "    final_resp = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=final_prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)         \n",
    "    ).text\n",
    "\n",
    "    return final_resp.strip()\n",
    "\n",
    "def extract_excel_features(code, year, df):\n",
    "    # build mask for matching row\n",
    "    mask = (df['Row Labels'] == code) & (df['Year'] == year)\n",
    "    matched = df.loc[mask]\n",
    "\n",
    "    # ensure exactly one match\n",
    "    n = len(matched)\n",
    "    if n == 0:\n",
    "        raise ValueError(f\"No rows found for code={code!r}, year={year}\")\n",
    "    if n > 1:\n",
    "        raise ValueError(f\"Expected 1 row but found {n} for code={code!r}, year={year}\")\n",
    "\n",
    "    # extract the single row as a Series\n",
    "    row = matched.iloc[0]\n",
    "\n",
    "    # pull out the desired fields\n",
    "    return {\n",
    "        'OpinionOnFinancialStatements':      row['OpinionOnFinancialStatements'],\n",
    "        'ValueformoneyArrangementsConclusion':         row['VfmArrangementsConclusion'],\n",
    "        'AdditionalReportingPowers':         row['AdditionalReportingPowers']\n",
    "    }\n",
    "\n",
    "\n",
    "# didn't use this part - used MemWalker instead of tf idf\n",
    "# def text_to_score(text):\n",
    "#     client = genai.Client(api_key=\"AIzaSyAs8WP1VWESVbfIZHARASewZO30j_r3tqY\")\n",
    "#     cleaned_text, features = preprocess_and_extract_features(text)\n",
    "#     chunks = split_overlapping_chunks(cleaned_text)\n",
    "#     top_indices = rank_chunks_tfidf(chunks, top_k=10)\n",
    "#     selected = [chunks[i-1] for i in top_indices]\n",
    "#     summaries = [summarize_chunk(c, client=client) for c in selected]\n",
    "#     final_summary = synthesize_summaries(summaries, client=client)\n",
    "#     score = summary_to_score(final_summary, features=features, client=client)\n",
    "#     return score\n",
    "\n",
    "def text_to_score(text, code, year, df):\n",
    "    client = genai.Client(api_key=KEY)\n",
    "    cleaned_text, features = preprocess_and_extract_features(text)\n",
    "    relevant_passages = memwalker_retrieve(cleaned_text, client=client)\n",
    "    final_input = \"\\n\\n\".join(relevant_passages)\n",
    "    if len(final_input) > 800*6:\n",
    "        final_input = summarize_full_text(final_input, client)\n",
    "    xl_f = extract_excel_features(code, year, df)\n",
    "    score = summary_to_score(final_input, features_1=features, features_2=xl_f, client=client)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa027e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: RL12017-2018.txt\n",
      "0.2\n",
      "Processing file: RQY2017-2018.txt\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"/Volumes/Z Slim/finaldataset?.xlsx\")\n",
    "\n",
    "folder_path = \"/Volumes/Z Slim/processed_reports/scanned_to_text\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    # Check if it's a file (and not a subfolder)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        code = filename[0:3]\n",
    "        year = filename[3:12]\n",
    "        assert filename[12:] == '.txt', f\"Error on file {filename}\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()    \n",
    "        financial_mismanagement_score = text_to_score(raw_text, code, year, df)\n",
    "        print(financial_mismanagement_score)\n",
    "        # Save score\n",
    "        mask = (df['Row Labels'] == code) & (df['Year'] == year)\n",
    "        row_count = mask.sum()  # Number of True values in mask\n",
    "        assert row_count == 1, f\"Expected exactly 1 matching row for (code={code}, year={year}), found {row_count}\"\n",
    "        df.loc[mask, 'financial mismangement score (0-1)'] = financial_mismanagement_score     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame saved\n"
     ]
    }
   ],
   "source": [
    "df.to_excel(\"/Volumes/Z Slim/finaldataset_with_iForest.xlsx\", index=False)\n",
    "print(f\"\\nDataFrame saved\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
